{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# set WORKDIR to the top of experiment repository\n",
    "%cd ..\n",
    "# %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from src.utils import timestamp\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.text_classification import ClassificationFunction\n",
    "import torch\n",
    "\n",
    "from src.settings import (\n",
    "    MLFLOW_TRACKING_USERNAME,\n",
    "    EXPERIMENT_NAME,\n",
    "    )\n",
    "\n",
    "\n",
    "# Check if a GPU is available and set the device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# select the model for evaluation\n",
    "all_models = {\n",
    "    'all-mpnet-base-v2': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'stsb-roberta-large': 'cross-encoder/stsb-roberta-large',\n",
    "    'stsb-roberta-base': 'cross-encoder/stsb-roberta-base',\n",
    "    'Legal-BERT': 'nlpaueb/legal-bert-base-uncased',\n",
    "    'EURLEX-BERT': 'nlpaueb/bert-base-uncased-eurlex',\n",
    "    'SciBERT': 'allenai/scibert_scivocab_uncased',\n",
    "    # fine-tuned models below\n",
    "    #\n",
    "}\n",
    "\n",
    "selected_model = all_models['stsb-roberta-large']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "dataset_path = 'data/test_clean.parquet'\n",
    "df_test = pd.read_parquet(dataset_path)\n",
    "\n",
    "# X\n",
    "sentence_pairs = list(zip(df_test['text'].tolist(),df_test['text_b'].tolist()))\n",
    "# sentence pairs as list of dicts for transformer's pipeline\n",
    "sentence_pairs_lods = [{\"text\": x[0], \"text_pair\": x[1]} for x in sentence_pairs]\n",
    "\n",
    "# y_true\n",
    "labels_true = df_test['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/31 11:18:21 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5100502512562815\n",
      "Matthews Correlation Coefficient: 0.0021392465789891314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/31 11:19:16 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/05/31 11:19:17 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "# init mlflow experiment (use existing one)\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "# load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(selected_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "\n",
    "# init mlflow experiment (use existing one)\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "# run experiment\n",
    "with mlflow.start_run(experiment_id=experiment.experiment_id, log_system_metrics=True) as run:\n",
    "    # set run name\n",
    "    mlflow.set_tag(key='mlflow.runName',\n",
    "                    value=f\"Evaluation_{selected_model.split('/')[1]}_{timestamp()}\")\n",
    "    \n",
    "    # log parameters\n",
    "    mlflow.log_params({\n",
    "        'PyTorch Device': torch.cuda.get_device_name(torch.cuda.current_device()),\n",
    "        'Model': selected_model,\n",
    "        'Dataset': dataset_path,\n",
    "        'AutoModel Parameters': model,\n",
    "        'Tokenizer': tokenizer,\n",
    "    })\n",
    "\n",
    "    # run pipeline for model predictions\n",
    "    pipe = pipeline(\"text-classification\", \n",
    "                    model = model, \n",
    "                    tokenizer = tokenizer, \n",
    "                    padding = True, \n",
    "                    truncation = True,\n",
    "                    # max_length = 512, \n",
    "                    device = device, \n",
    "                    # function_to_apply = ClassificationFunction.SIGMOID,\n",
    "                    # top_k=None,\n",
    "                    )\n",
    "\n",
    "    predictions = pipe(sentence_pairs_lods)\n",
    "\n",
    "    # binarization for cross-encoders\n",
    "    if selected_model.split('/')[0] in ['cross-encoder']:\n",
    "        # for threshold in [\n",
    "        #     0.33,\n",
    "        #     0.53,\n",
    "        #     0.66,\n",
    "        #     0.75,\n",
    "        #     0.85,\n",
    "        # ]:\n",
    "        # threshold\n",
    "        threshold = 0.54\n",
    "        labels_pred = [0 if x['score'] <= threshold else 1 for x in predictions]\n",
    "\n",
    "    else:\n",
    "        labels_pred = [0 if x['label'] == 'LABEL_0' else 1 for x in predictions]\n",
    "\n",
    "    f1_score_value = f1_score(y_true=labels_true, y_pred=labels_pred, pos_label=1, average='binary')\n",
    "    mlflow.log_metric(\"F1 Score\", f1_score_value)\n",
    "\n",
    "    matthews_corrcoef_value = matthews_corrcoef(y_true=labels_true, y_pred=labels_pred)\n",
    "    mlflow.log_metric(\"Matthews Correlation Coefficient\", matthews_corrcoef_value)\n",
    "\n",
    "    print(f\"F1 Score: {f1_score_value}\\nMatthews Correlation Coefficient: {matthews_corrcoef_value}\")\n",
    "\n",
    "# end experiment\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mlflow\n",
    "- Implement a classic transformer-based classifier.\n",
    "    - Split the training data into train and validation sets\n",
    "    - fine-tune\n",
    "    - test with test data\n",
    "- Obtain quality metrics on the test set. Do include F1 score and Matthews Correlation Coefficient https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset(\"parquet\", data_files={\"train\": \"data/train_clean.parquet\", \"test\": \"data/test_clean.parquet\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
