{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set WORKDIR to the top of experiment repository\n",
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from src.settings import (\n",
    "    MLFLOW_TRACKING_USERNAME,\n",
    "    EXPERIMENT_NAME,\n",
    "    )\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    \"\"\"This function creates current timestamp\"\"\"\n",
    "    return datetime.now().strftime(\"%Y_%m_%d%H_%M_%S\")\n",
    "\n",
    "\n",
    "# select the model for evaluation\n",
    "all_models = {\n",
    "    'all-mpnet-base-v2': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'stsb-roberta-large': 'cross-encoder/stsb-roberta-large',\n",
    "    'stsb-roberta-base': 'cross-encoder/stsb-roberta-base',\n",
    "    'Legal-BERT': 'nlpaueb/legal-bert-base-uncased',\n",
    "    'EURLEX-BERT': 'nlpaueb/bert-base-uncased-eurlex',\n",
    "    'SciBERT': 'allenai/scibert_scivocab_uncased',\n",
    "}\n",
    "\n",
    "selected_model = all_models['stsb-roberta-base']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "df_test = pd.read_parquet('data/test_clean.parquet')\n",
    "\n",
    "sentence_pairs = list(zip(df_test['text'].tolist(),df_test['text_b'].tolist()))\n",
    "sentence_pairs_lds = [{\"text\": x[0], \"text_pair\": x[1]} for x in sentence_pairs]\n",
    "\n",
    "labels_true = df_test['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.10206207261596577\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "\n",
    "# Load a pre-trained CrossEncoder model\n",
    "model = CrossEncoder(selected_model)\n",
    "\n",
    "# Predict scores for a pair of sentences\n",
    "scores = model.predict(sentence_pairs)\n",
    "\n",
    "# scores to binary\n",
    "# threshold = 0.45\n",
    "for threshold in [\n",
    "    0.51,\n",
    "    0.60,\n",
    "    0.75,\n",
    "    0.85,\n",
    "    0.90,\n",
    "]:\n",
    "    labels_pred = [0 if x <= threshold else 1 for x in scores]\n",
    "    matthews_corrcoef_values = matthews_corrcoef(y_true=labels_true, y_pred=labels_pred)\n",
    "    print(matthews_corrcoef_values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save score predictions\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores.to_parquet('data/stsb-roberta-base_pretrain_test_scores.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(selected_model)\n",
    "\n",
    "scores = list()\n",
    "\n",
    "for sentence_pair in sentence_pairs:\n",
    "    # Example pair of legal texts\n",
    "    text1 = sentence_pair[0]\n",
    "    text2 = sentence_pair[1]\n",
    "\n",
    "    # Tokenize the texts\n",
    "    inputs = tokenizer(text1, text2, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # Get model predictions\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Get the score (e.g., similarity score)\n",
    "    score = torch.softmax(logits, dim=1)\n",
    "\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3469],\n",
      "        [-0.4552],\n",
      "        [-0.3044],\n",
      "        [-0.1127],\n",
      "        [ 0.3391],\n",
      "        [-0.5461],\n",
      "        [ 0.1714],\n",
      "        [ 0.3955],\n",
      "        [ 0.6687],\n",
      "        [ 0.5301]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(selected_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "\n",
    "features = tokenizer(sentence_pairs[:10], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**features).logits\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.41412752866744995},\n",
       " {'label': 'LABEL_0', 'score': 0.38811999559402466},\n",
       " {'label': 'LABEL_0', 'score': 0.4244891107082367},\n",
       " {'label': 'LABEL_0', 'score': 0.4718495309352875},\n",
       " {'label': 'LABEL_0', 'score': 0.5839836597442627},\n",
       " {'label': 'LABEL_0', 'score': 0.3667718768119812},\n",
       " {'label': 'LABEL_0', 'score': 0.5427504181861877},\n",
       " {'label': 'LABEL_0', 'score': 0.5976123213768005},\n",
       " {'label': 'LABEL_0', 'score': 0.6612198352813721},\n",
       " {'label': 'LABEL_0', 'score': 0.629508376121521}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.text_classification import ClassificationFunction\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(selected_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, function_to_apply=ClassificationFunction.SIGMOID)\n",
    "\n",
    "\n",
    "pipe(sentence_pairs_lds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
