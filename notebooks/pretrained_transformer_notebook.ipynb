{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set WORKDIR to the top of experiment repository\n",
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import pandas as pd\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "from src.settings import (\n",
    "    MLFLOW_TRACKING_USERNAME,\n",
    "    EXPERIMENT_NAME,\n",
    "    )\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    \"\"\"This function creates current timestamp\"\"\"\n",
    "    return datetime.now().strftime(\"%Y_%m_%d%H_%M_%S\")\n",
    "\n",
    "\n",
    "# select the model for evaluation\n",
    "all_models = {\n",
    "    'all-mpnet-base-v2': 'sentence-transformers/all-mpnet-base-v2',\n",
    "    'stsb-roberta-large': 'cross-encoder/stsb-roberta-large',\n",
    "    'stsb-roberta-base': 'cross-encoder/stsb-roberta-base',\n",
    "    'Legal-BERT': 'nlpaueb/legal-bert-base-uncased',\n",
    "    'EURLEX-BERT': 'nlpaueb/bert-base-uncased-eurlex',\n",
    "    'SciBERT': 'allenai/scibert_scivocab_uncased',\n",
    "}\n",
    "\n",
    "selected_model = all_models['Legal-BERT']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "df_test = pd.read_parquet('data/test_clean.parquet')\n",
    "\n",
    "sentence_pairs = list(zip(df_test['text'].tolist(),df_test['text_b'].tolist()))\n",
    "sentence_pairs_lds = [{\"text\": x[0], \"text_pair\": x[1]} for x in sentence_pairs]\n",
    "\n",
    "labels_true = df_test['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/data_science/projects/LOGOSAI.TECH_external/CHALLENGE/Experiment-repository-template/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained CrossEncoder model\n",
    "model = CrossEncoder(selected_model, num_labels=1)\n",
    "\n",
    "# Predict scores for a pair of sentences\n",
    "scores = model.predict(sentence_pairs)\n",
    "scores = model.predict(\n",
    "    [\n",
    "        (,),\n",
    "        (,),\n",
    "        (,),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52866924, 0.5696899 , 0.5411513 , 0.56135166, 0.51937073,\n",
       "       0.5359785 , 0.49534476, 0.5971306 , 0.45699373, 0.4617223 ,\n",
       "       0.5254464 , 0.55498594, 0.5865344 , 0.5382738 , 0.5769908 ,\n",
       "       0.45926172, 0.5379857 , 0.5193613 , 0.5873433 , 0.53996336,\n",
       "       0.57705367, 0.52027017, 0.6214167 , 0.58687866, 0.47739467,\n",
       "       0.5033362 , 0.556111  , 0.5483626 , 0.5360353 , 0.5114348 ,\n",
       "       0.54978013, 0.512177  , 0.5754094 , 0.5375439 , 0.58661795,\n",
       "       0.4572795 , 0.5442641 , 0.4586027 , 0.48504   , 0.56655174,\n",
       "       0.5579274 , 0.51000935, 0.58568335, 0.558887  , 0.45419514,\n",
       "       0.47232154, 0.5408257 , 0.5084745 , 0.5686562 , 0.55643755,\n",
       "       0.5692722 , 0.5502371 , 0.4520715 , 0.58603734, 0.5612508 ,\n",
       "       0.5666476 , 0.53557783, 0.48357564, 0.49957252, 0.46137533,\n",
       "       0.4696086 , 0.5496542 , 0.46850467, 0.46216702, 0.55400074,\n",
       "       0.57808346, 0.53461474, 0.47380352, 0.577394  , 0.5317137 ,\n",
       "       0.47649798, 0.5466648 , 0.57183444, 0.58767444, 0.56543475,\n",
       "       0.53306496, 0.5234752 , 0.4662387 , 0.5723316 , 0.5165427 ,\n",
       "       0.5263119 , 0.43399224, 0.47965512, 0.53555137, 0.47353575,\n",
       "       0.579921  , 0.5662732 , 0.4283526 , 0.48196977, 0.54455215,\n",
       "       0.47220275, 0.5143684 , 0.56041753, 0.50380677, 0.48027644,\n",
       "       0.58064294, 0.556106  , 0.5705497 , 0.488897  , 0.42913863,\n",
       "       0.5787233 , 0.5784805 , 0.4792305 , 0.566015  , 0.54899323,\n",
       "       0.5119763 , 0.53566414, 0.5204536 , 0.45893717, 0.5784962 ,\n",
       "       0.5511062 , 0.5322579 , 0.50235367, 0.5788612 , 0.59030324,\n",
       "       0.51259154, 0.6046171 , 0.48849374, 0.5281901 , 0.43152687,\n",
       "       0.5164475 , 0.49610806, 0.5479139 , 0.49373278, 0.49894923,\n",
       "       0.49541023, 0.52731913, 0.5414688 , 0.51563746, 0.4834314 ,\n",
       "       0.5426752 , 0.5671872 , 0.45773783, 0.5654512 , 0.46093643,\n",
       "       0.5695412 , 0.580493  , 0.5856556 , 0.5039    , 0.50437903,\n",
       "       0.4813414 , 0.4741071 , 0.5415489 , 0.565302  , 0.47626972,\n",
       "       0.53621864, 0.5352083 , 0.4469239 , 0.5843908 , 0.5995525 ,\n",
       "       0.49305055, 0.57401925, 0.48809007, 0.5747193 , 0.57174987,\n",
       "       0.45764104, 0.60243404, 0.5697018 , 0.58238006, 0.49714643,\n",
       "       0.4847364 , 0.5485919 , 0.48162925, 0.4914341 , 0.54357713,\n",
       "       0.54659104, 0.47117662, 0.547422  , 0.5333804 , 0.47000563,\n",
       "       0.53200114, 0.49121705, 0.60830617, 0.48558572, 0.49806187,\n",
       "       0.56960857, 0.54324585, 0.5464245 , 0.4754346 , 0.5721251 ,\n",
       "       0.4826523 , 0.48348412, 0.5609005 , 0.58137715, 0.56451344,\n",
       "       0.44445142, 0.43581587, 0.44303504, 0.56698495, 0.54413366,\n",
       "       0.54733634, 0.5868965 , 0.46455032, 0.5293297 , 0.5270837 ,\n",
       "       0.5365318 , 0.5276801 , 0.5832451 , 0.5766406 , 0.52974784,\n",
       "       0.57208246, 0.5770508 , 0.5625939 , 0.46123728, 0.56835365,\n",
       "       0.4753465 , 0.5402365 , 0.5750859 , 0.4741743 , 0.48316213,\n",
       "       0.45874128, 0.5476988 , 0.46814805, 0.5669595 , 0.57558405,\n",
       "       0.5263377 , 0.61275023, 0.41595253, 0.50781983, 0.5317435 ,\n",
       "       0.5290862 , 0.4941533 , 0.57276976, 0.5060391 , 0.5093139 ,\n",
       "       0.530945  , 0.52288634, 0.55604327, 0.48556256, 0.52689886,\n",
       "       0.43942338, 0.51966625, 0.5495098 , 0.49425706, 0.45529285,\n",
       "       0.48613188, 0.46626773, 0.54637814, 0.53554   , 0.55469674,\n",
       "       0.58645034, 0.45833138, 0.5848916 , 0.51003987, 0.57919776,\n",
       "       0.47135857, 0.5500768 , 0.4728728 , 0.54424393, 0.5702373 ,\n",
       "       0.56634384, 0.5509163 , 0.5747929 , 0.5712983 , 0.47928306,\n",
       "       0.5819304 , 0.51777726, 0.57426035, 0.45669767, 0.5678963 ,\n",
       "       0.5430076 , 0.54484457, 0.5644117 , 0.4787946 , 0.58251774,\n",
       "       0.4775471 , 0.511981  , 0.47935298, 0.51562965, 0.53138745,\n",
       "       0.51737934, 0.4753511 , 0.43293887, 0.4797709 , 0.5562604 ,\n",
       "       0.51753867, 0.50951695, 0.43482512, 0.5949814 , 0.56872714,\n",
       "       0.5230695 , 0.5768968 , 0.58363664, 0.5720217 , 0.56631374,\n",
       "       0.44422567, 0.5949266 , 0.46220472, 0.60929173, 0.5667222 ,\n",
       "       0.56264025, 0.57244843, 0.57272524, 0.560334  , 0.55264   ,\n",
       "       0.45495927, 0.5163847 , 0.5316339 , 0.5695952 , 0.489941  ,\n",
       "       0.44375497, 0.56923866, 0.5007326 , 0.5384895 , 0.46866685,\n",
       "       0.49783793, 0.48962522, 0.50744355, 0.50566083, 0.5405811 ,\n",
       "       0.5068068 , 0.44734824, 0.54322314, 0.51061416, 0.5244189 ,\n",
       "       0.54357815, 0.524573  , 0.5543966 , 0.5798495 , 0.47266698,\n",
       "       0.5120694 , 0.42798364, 0.58684593, 0.58385104, 0.49197024,\n",
       "       0.5047793 , 0.559648  , 0.576681  , 0.48827514, 0.5647262 ,\n",
       "       0.5705715 , 0.5619475 , 0.5878652 , 0.58961695, 0.5938908 ,\n",
       "       0.5377559 , 0.54487216, 0.587237  , 0.5532061 , 0.56320494,\n",
       "       0.5250522 , 0.5447948 , 0.5211312 , 0.5554445 , 0.4790639 ,\n",
       "       0.49177492, 0.58388245, 0.58555865, 0.48069763, 0.58520293,\n",
       "       0.5773629 , 0.48200065, 0.5725253 , 0.5752569 , 0.5476765 ,\n",
       "       0.54706115, 0.46351883, 0.54921275, 0.5302842 , 0.57174754,\n",
       "       0.56570435, 0.5254452 , 0.5424811 , 0.5722406 , 0.5181105 ,\n",
       "       0.4972663 , 0.47633198, 0.523392  , 0.5762623 , 0.56878054,\n",
       "       0.5223836 , 0.49809894, 0.55659324, 0.549098  , 0.45539296,\n",
       "       0.5135889 , 0.46003643, 0.47295725, 0.5037974 , 0.5140917 ,\n",
       "       0.5730971 , 0.5150039 , 0.4580447 , 0.5328042 , 0.57946724,\n",
       "       0.57957625, 0.565654  , 0.5724467 , 0.5557812 , 0.48517367,\n",
       "       0.5506527 , 0.49755973, 0.49459842, 0.51198983, 0.5520478 ,\n",
       "       0.47097492, 0.5601988 , 0.47222108, 0.5765162 , 0.5375943 ,\n",
       "       0.5054715 , 0.55940825, 0.58547634, 0.5605695 , 0.60218084,\n",
       "       0.44642767, 0.56574   , 0.55113375, 0.5726871 , 0.56264377,\n",
       "       0.4742499 , 0.56884474, 0.47723055, 0.5739504 , 0.54667735,\n",
       "       0.5092874 , 0.541666  , 0.48289946, 0.5383556 , 0.57842696,\n",
       "       0.44911546, 0.51468384, 0.43694416, 0.49430472, 0.5367504 ,\n",
       "       0.5859417 , 0.4454553 , 0.54491675, 0.5689662 , 0.57826287,\n",
       "       0.46338692, 0.45998266, 0.5895692 , 0.5250148 , 0.5788559 ,\n",
       "       0.5953694 , 0.46749482, 0.5417    , 0.5591024 , 0.4958581 ,\n",
       "       0.47887632, 0.54680955, 0.53260344, 0.5862557 , 0.5228198 ,\n",
       "       0.58128655, 0.53641766, 0.5031852 , 0.57598794, 0.53958595,\n",
       "       0.54629076, 0.5655355 , 0.5813131 , 0.4946736 , 0.5669866 ,\n",
       "       0.5229743 , 0.44063526, 0.49540913, 0.48717445, 0.5780343 ,\n",
       "       0.55840373, 0.55704236, 0.53901124, 0.56309444, 0.54099184,\n",
       "       0.45941103, 0.5158841 , 0.5433848 , 0.5776541 , 0.5219124 ,\n",
       "       0.5583679 , 0.53260946, 0.4638943 , 0.5481152 , 0.5709464 ,\n",
       "       0.50924164, 0.5635698 , 0.553006  , 0.5744221 , 0.48690265,\n",
       "       0.5781989 , 0.525282  , 0.4738503 , 0.57366383, 0.55996484,\n",
       "       0.60511947, 0.4851883 , 0.53173625, 0.5792618 , 0.5376568 ,\n",
       "       0.5549552 , 0.4901762 , 0.5668017 , 0.5362958 , 0.48682013,\n",
       "       0.46693537, 0.53059673, 0.49218994, 0.58395344, 0.5563388 ,\n",
       "       0.56850064, 0.4379787 , 0.5478498 , 0.58065665, 0.5138346 ,\n",
       "       0.5251613 , 0.49232966, 0.50140584, 0.4992182 , 0.5772501 ,\n",
       "       0.5338998 , 0.513731  , 0.5051765 , 0.5558175 , 0.47474074,\n",
       "       0.54528624, 0.48833528, 0.52083087, 0.46583378, 0.5372465 ,\n",
       "       0.5752678 , 0.56179416, 0.5471604 , 0.56741303, 0.48452643,\n",
       "       0.5938755 , 0.51331365, 0.5767112 , 0.5541149 , 0.64128196,\n",
       "       0.5016926 , 0.5202465 , 0.5656784 , 0.56633943, 0.58255106,\n",
       "       0.4913594 , 0.4834043 , 0.46732557, 0.58650583, 0.53512084,\n",
       "       0.5517079 , 0.5964497 , 0.58866894, 0.5684082 , 0.4813867 ,\n",
       "       0.52960837, 0.5523828 , 0.6122149 , 0.54547507, 0.46981916,\n",
       "       0.46813017, 0.5591939 , 0.5301535 , 0.48504898, 0.47820768,\n",
       "       0.4941763 , 0.50611085, 0.49025786, 0.5680652 , 0.5634254 ,\n",
       "       0.4883217 , 0.5718767 , 0.5241741 , 0.57449096, 0.5222981 ,\n",
       "       0.53764725, 0.5476524 , 0.60604435, 0.56107503, 0.5463458 ,\n",
       "       0.5493686 , 0.6023303 , 0.57088256, 0.5033815 , 0.53816974,\n",
       "       0.5941702 , 0.563865  , 0.56459385, 0.50879705, 0.5399399 ,\n",
       "       0.5924815 , 0.5232875 , 0.53322697, 0.57786393, 0.5455428 ,\n",
       "       0.48879883, 0.4964739 , 0.5860795 , 0.45379433, 0.47485888,\n",
       "       0.5339749 , 0.47944963, 0.54775906, 0.5334257 , 0.589317  ,\n",
       "       0.57919383, 0.48506945, 0.54066354, 0.4575389 , 0.46231166,\n",
       "       0.58600944, 0.47909826, 0.48971018, 0.5271705 , 0.5239521 ,\n",
       "       0.45159897, 0.61900026, 0.48939124, 0.57711273, 0.6109697 ,\n",
       "       0.5689632 , 0.53311974, 0.5108366 , 0.47760165, 0.47605002,\n",
       "       0.48582172, 0.4924744 , 0.64981765, 0.40065092, 0.5750827 ,\n",
       "       0.48259512, 0.5688528 , 0.58023375, 0.44980723, 0.60939974,\n",
       "       0.58403176, 0.5145505 , 0.4860653 , 0.5045903 , 0.5764552 ,\n",
       "       0.4727729 , 0.4663203 , 0.4787111 , 0.60485387, 0.55986315,\n",
       "       0.4629139 , 0.57063156, 0.4473461 , 0.5694616 , 0.531697  ,\n",
       "       0.5064722 , 0.5682322 , 0.5084623 , 0.45385823, 0.52904993,\n",
       "       0.46082193, 0.5253581 , 0.5388672 , 0.5846702 , 0.52332026,\n",
       "       0.5494805 , 0.44749367, 0.508961  , 0.6142888 , 0.5700015 ,\n",
       "       0.551512  , 0.5655665 , 0.49803388, 0.52261305, 0.58424455,\n",
       "       0.4521913 , 0.5090491 , 0.45658126, 0.4809254 , 0.5471749 ,\n",
       "       0.5264444 , 0.5146185 , 0.4854184 , 0.5009508 , 0.5697702 ,\n",
       "       0.48386988, 0.50567126, 0.49700946, 0.5674455 , 0.56469566,\n",
       "       0.50776184, 0.5698232 , 0.5168195 , 0.576485  , 0.5698946 ,\n",
       "       0.5359308 , 0.54701716, 0.5266584 , 0.43730953, 0.4937944 ,\n",
       "       0.5015992 , 0.46897814, 0.5379114 , 0.552087  , 0.5957504 ,\n",
       "       0.5607153 , 0.54442906, 0.46245122, 0.5812922 , 0.4854378 ,\n",
       "       0.561669  , 0.522149  , 0.50368774, 0.5478351 , 0.55743134,\n",
       "       0.47185144, 0.57838404, 0.57542485, 0.55343705, 0.56479836,\n",
       "       0.4991972 , 0.46192074, 0.4483932 , 0.53190225, 0.48694888,\n",
       "       0.5512105 , 0.48444852, 0.583801  , 0.5456013 , 0.5546403 ,\n",
       "       0.47905672, 0.5130376 , 0.48992467, 0.5511564 , 0.5916422 ,\n",
       "       0.50257677, 0.58319694, 0.51218647, 0.41837683, 0.5623258 ,\n",
       "       0.5889661 , 0.5216455 , 0.4852373 , 0.55718714, 0.44759825,\n",
       "       0.57804865, 0.5662308 , 0.63767207, 0.57537955, 0.58274204,\n",
       "       0.5502528 , 0.5410861 , 0.57576656, 0.5158915 , 0.485597  ,\n",
       "       0.59523064, 0.51905805, 0.57640755, 0.4715293 , 0.5809932 ,\n",
       "       0.52131385, 0.5665331 , 0.5422953 , 0.5723535 , 0.6305451 ,\n",
       "       0.5838908 , 0.5275068 , 0.5803552 , 0.49879497, 0.43766356,\n",
       "       0.56312895, 0.57374436, 0.55727094, 0.51379067, 0.48489738,\n",
       "       0.58045924, 0.46953037, 0.53037685, 0.48942178, 0.5263822 ,\n",
       "       0.6063672 , 0.4392405 , 0.53207093], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017501160638229624\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# scores to binary\n",
    "# threshold = 0.45\n",
    "for threshold in [\n",
    "    0.53,\n",
    "    0.66,\n",
    "    0.75,\n",
    "    0.85,\n",
    "    0.90,\n",
    "]:\n",
    "    labels_pred = [0 if x <= threshold else 1 for x in scores]\n",
    "    matthews_corrcoef_values = matthews_corrcoef(y_true=labels_true, y_pred=labels_pred)\n",
    "    print(matthews_corrcoef_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save score predictions\n",
    "# df_scores = pd.DataFrame(scores)\n",
    "# df_scores.to_parquet('data/stsb-roberta-base_pretrain_test_scores.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(selected_model)\n",
    "\n",
    "# scores = list()\n",
    "\n",
    "# for sentence_pair in sentence_pairs:\n",
    "#     # Example pair of legal texts\n",
    "#     text1 = sentence_pair[0]\n",
    "#     text2 = sentence_pair[1]\n",
    "\n",
    "#     # Tokenize the texts\n",
    "#     inputs = tokenizer(text1, text2, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "#     # Get model predictions\n",
    "#     outputs = model(**inputs)\n",
    "#     logits = outputs.logits\n",
    "\n",
    "#     # Get the score (e.g., similarity score)\n",
    "#     score = torch.softmax(logits, dim=1)\n",
    "\n",
    "#     scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import torch\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(selected_model)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "\n",
    "# features = tokenizer(sentence_pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     scores = model(**features).logits\n",
    "#     print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test_3 with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.text_classification import ClassificationFunction\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available and set the device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(selected_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(selected_model)\n",
    "\n",
    "pipe = pipeline(\"text-classification\", \n",
    "                model = model, \n",
    "                tokenizer = tokenizer, \n",
    "                padding = True, \n",
    "                truncation = True,\n",
    "                # max_length = 512, \n",
    "                device = device, \n",
    "                function_to_apply = ClassificationFunction.SIGMOID,\n",
    "                )\n",
    "\n",
    "predictions = pipe(sentence_pairs_lds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarization\n",
    "for threshold in [\n",
    "    0.53,\n",
    "    0.66,\n",
    "    0.75,\n",
    "    0.85,\n",
    "    0.90,\n",
    "]:\n",
    "    labels_pred = [0 if x['score'] <= threshold else 1 for x in predictions]\n",
    "    matthews_corrcoef_values = matthews_corrcoef(y_true=labels_true, y_pred=labels_pred)\n",
    "    print(matthews_corrcoef_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytor_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
